# Paper Proposals

| Paper | Piece of abstract |
| ----- | ----------------- |
| [A Tutorial on Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf) | > Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box.|
| [Differentially Private Federated Learning: A Client Level Perspective](https://arxiv.org/pdf/1712.07557.pdf) | > Federated learning is a recent advance in privacy protection. In this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. |
| [Neural Turing Machine](https://arxiv.org/pdf/1410.5401.pdf) | > We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. |
| [Gradient-Based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)| > Multilayer Neural Netoworks trained with the backpropagation algorithm constitude the best example of a succesful Gradient-Based Learning technique. |
| ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938v3.pdf)| > Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.|
| [Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/pdf/1711.11279.pdf)| > In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. |
| [An Efficient Two-Layer Mechanism for Privacy-Preserving Truth Discovery](https://www.kdd.org/kdd2018/accepted-papers/view/an-efficient-two-layer-mechanism-for-privacy-preserving-truth-discovery)| > Soliciting answers from online users is an efficient and effective solution to many challenging tasks. Due to the variety in the quality of users, it is important to infer their ability to provide correct answers during aggregation. |
| [Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805v1.pdf) | > We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. |
| [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) | > We propose two novel model architectures for computing continuous vector representations of words from very large data sets.|
| [A fast learning algorithm for deep belief nets*](http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf) | > We show how to use “complementary priors” to eliminate the explaining away effects that make inference difficult in densely-connected belief nets that have many hidden layers. |
| [Reducing the dimensionality of data with neural networks](http://www.cs.toronto.edu/~hinton/absps/science_som.pdf) | |
| [Deep Learning for Sentiment Analysis: A Survey](https://arxiv.org/ftp/arxiv/papers/1801/1801.07883.pdf) | > Deep	learning has emerged as	a powerful machine learning technique that learns multiple layers of	 representations	or features	of the data and	produces state-of-the-art prediction	results. |
| [Understanding Batch Normalization](https://arxiv.org/pdf/1806.02375.pdf) | > Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. |
| [Study of Twitter Sentiment Analysis using Machine Learning Algorithms on Python](https://pdfs.semanticscholar.org/c114/7f3d9b46ff0a0c7c43b668123cb15a26120d.pdf) | > Twitter is a platform widely used by people to express their opinions and display sentiments on different occasions. Sentiment analysis is an approach to analyze data and retrieve sentiment that it embodies. |
| [SEMI-SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNING FROM PRIVATE TRAINING DATA](https://arxiv.org/pdf/1610.05755.pdf) | > Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. |
| [A generic framework for privacy preserving deep learning](https://arxiv.org/pdf/1811.04017.pdf!) | > We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. |
| [Robust De-anonymization of Large Sparse Datasets](https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf) | > We present a new class of statistical deanonymization attacks against high-dimensional micro-data, such as individual preferences, recommendations, transaction records and so on. Our techniques are robust to perturbation in the data and tolerate some mistakes in the adversary’s background knowledge. |
| [Differential Privacy](https://www.utdallas.edu/~muratk/courses/privacy08f_files/differential-privacy.pdf) | > In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. |
| [Image Super-Resolution Using Deep Convolutional Networks](https://arxiv.org/pdf/1501.00092.pdf) | > We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. |
| [SecureNN: 3-Party Secure Computation for Neural Network Training](https://eprint.iacr.org/2018/442.pdf) | > Neural Networks (NN) provide a powerful method for machine learning training and inference. To effectively train, it is desirable for multiple parties to combine their data – however, doing so conflicts with data privacy. |
| [DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-based Incentive](https://eprint.iacr.org/2018/679.pdf) | > Recently, privacy-preserving deep learning has drawn tremendous attention from information security community, in which neither training data nor the training model is expected to be exposed. |
| [Decoding individual differences in STEM learning from functional MRI data](https://www.nature.com/articles/s41467-019-10053-y) | > Here, we investigated whether patterns of brain activity collected during a concept knowledge task could be used to compute a neural ‘score’ to complement traditional scores of an individual’s conceptual understanding. |
| [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) | > We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. |
